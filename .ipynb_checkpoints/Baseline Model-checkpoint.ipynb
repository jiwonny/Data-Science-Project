{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tell whether our recommendation system is working well on data or not, we should have a baseline model to compare with. \n",
    "\n",
    "* Here, we adopted a popularity model that recommends top-5 most purchased product to users.\n",
    "\n",
    "- Our plan for building a popularity baseline model is like this.\n",
    "    * load the train & validation data and preprocess the data\n",
    "    * build a train matrix with top-5 Product_ID and User_ID.\n",
    "    * build a matrix with the same matrix frame with the step 1's but with all entries 1.\n",
    "    * use similarity between 2 vectors of 5-dimensionality below\n",
    "        - one with the recommendation result matrix,which is step 3 matrix - step 2 matrix\n",
    "            * the result matrix would have entry with 1 only when the user didn't buy the product in train data but now they got recommended.\n",
    "        - the other with the validation martix with entry only 1 when user bought the product in validation set\n",
    "            * this would act as like a answer.\n",
    "         - cf. we exclude and keep our test data unexposed just in case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the train and validation data\n",
    "- These pre-splitted train and validation data is exactly the same with the data set we used to build our own recommendation model.\n",
    "    * We split train and validation of a user who bought more than 300 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "# load the validation data\n",
    "val = pd.read_csv(\"val_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only need User_ID,Product_ID, and purchased to build a baseline model, we'll drop all other columns here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns of a train data\n",
    "train = train.drop(['Unnamed: 0', 'index', 'countProduct'], axis=1)\n",
    "#drop columns of a validation data\n",
    "val = val.drop(['Unnamed: 0', 'index', 'countProduct'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the introduction of building this baseline model, we planned to recommend the top 5 most-frequently pruchased items to all users. This indicates that we don't need any rows with the Product_ID that's not in the list of top-5 Product_ID. For this deletion step, firstly, we need to know the which items are top-5 things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Top-5 most frequently-purchased items & preprocessing\n",
    "Here, we are going to discover 5 items that were most frequently purchased. To do this, we refer to the code we wrote in the very first of our data pipeline step, exploratory data analysis. Only difference is here we use top-5 but before, we had top-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P00265242    1858\n",
       "P00110742    1591\n",
       "P00025442    1586\n",
       "P00112142    1539\n",
       "P00057642    1430\n",
       "Name: Product_ID, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load an original data set\n",
    "origin = pd.read_csv(\"BlackFriday.csv\")\n",
    "#top-5 poducts sold\n",
    "origin[\"Product_ID\"].value_counts(sort=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those 5 Product_IDs are from the very original data set.\n",
    "Now, it's time to delete all the rows with non-top-5 items!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_1 = train[(train.Product_ID == 'P00265242')]\n",
    "new_train_2 = train[(train.Product_ID == 'P00110742')]\n",
    "new_train_3 = train[(train.Product_ID == 'P00025442')]\n",
    "new_train_4 = train[(train.Product_ID == 'P00112142')]\n",
    "new_train_5 = train[(train.Product_ID == 'P00057642')]\n",
    "new_train = pd.concat([new_train_1, new_train_2, new_train_3, new_train_4, new_train_5])\n",
    "\n",
    "new_val_1 = val[(val.Product_ID == 'P00265242')]\n",
    "new_val_2 = val[(val.Product_ID == 'P00110742')]\n",
    "new_val_3 = val[(val.Product_ID == 'P00025442')]\n",
    "new_val_4 = val[(val.Product_ID == 'P00112142')]\n",
    "new_val_5 = val[(val.Product_ID == 'P00057642')]\n",
    "new_val = pd.concat([new_val_1, new_val_2, new_val_3, new_val_4, new_val_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we got a same dataframe of User_ID and top-5 items for the train and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. build a matrix\n",
    "Here, we are going to transfrom the dataframe above into the matrix of User_ID and 5 Product_ID with the entry 0 or 1. 0 means the user didn't purchased the item or get the recommendation. On the other hand, 1 means the user bought the item or get the recommendation.\n",
    "- Matrices we need & meaning\n",
    "    * train matrix : shows the purchased history of users for the 5 items.\n",
    "    * recommendation matrix : matrix with all 1 - train matrix \n",
    "    -> recommending top 5 items to all users in train matrix\n",
    "           1 for the recommend the item, 0 for no recommendation for the item since the user already bought it.\n",
    "    * validation matrix : kind of an answer matrix.\n",
    " - compute cosine similarity of each user vector of recommendation matrix & validation matrix if the user exists both in two matrices.\n",
    " - that would act as an accuracy of our baseline model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a train matrix\n",
    "train_matrix = pd.pivot_table(new_train, values='purchased', index='User_ID', columns='Product_ID', fill_value=0)\n",
    "#budling a validation matrix\n",
    "val_matrix = pd.pivot_table(new_val, values='purchased', index='User_ID', columns='Product_ID', fill_value=0)\n",
    "#building a matrix with all entries 1\n",
    "recom_matrix = pd.pivot_table(new_train, index='User_ID', columns='Product_ID', fill_value=1)\n",
    "\n",
    "#gain a recommendation matrix\n",
    "recom_matrix = recom_matrix - train_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we got a recommendation matrix! \n",
    "* entry of 1 : recommend the item to the user since user didn't purchase it in the train matrix.\n",
    "* entry of 0 : do not recommend the item to the user since user already bought it in the train matrix.\n",
    "- So, to sum up, we only recommend the item to the user in the case the user didn't bought the item before(train set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. implement a consine similarity function & compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "    return dot(A,B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep the User_ID of train matrix & top-5 items' Product_ID as a dataframe\n",
    "users = pd.DataFrame(new_train.User_ID.unique())\n",
    "users.columns = ['User_ID']\n",
    "users_length = len(users)\n",
    "\n",
    "top5products = pd.DataFrame(new_val.Product_ID.unique())\n",
    "top5products.columns = ['Product_ID']\n",
    "top5products_length = len(top5products)\n",
    "\n",
    "#keep the User_ID of validation matrix\n",
    "val_user_info = pd.DataFrame(new_val.User_ID)\n",
    "val_user_info.columns = ['User_ID']\n",
    "\n",
    "val_users_length = len(val_user_info)\n",
    "similarity_matrix = np.zeros(shape=(val_users_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a similarity between user vectors of 5 dimesionality for the user who exist both in recommendation & validation matrix\n",
    "for i, user1 in enumerate(val_user_info['User_ID']):\n",
    "    for user2 in recom_matrix.index:\n",
    "        if(user1 == user2):\n",
    "            user_recom = np.array(recom_matrix.loc[user1])\n",
    "            user_val = np.array(val_matrix.loc[user2])\n",
    "            \n",
    "            similarity_matrix[i] = cos_sim(user_recom, user_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.585168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.122014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  18.000000\n",
       "mean    0.585168\n",
       "std     0.122014\n",
       "min     0.500000\n",
       "25%     0.500000\n",
       "50%     0.577350\n",
       "75%     0.577350\n",
       "max     1.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(similarity_matrix).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This would act like a baseline model's accuracy to compare with the accuracy of our new recommendation model.\n",
    "- Here, we can see the similarity 1 the highest value obtainable, min  0.5.\n",
    "- We guess this would be somehow already very well-built model for baseline model with the sound similarity matrix, the accuracy. However, since we had some size difference between recommendation matrix and validation matrix and also it was just for the top-5 items it could fall behind the model we'd built in the next step or even do better than it with far more data. \n",
    "- Comparison with this baseline model and new recommendation model will come after building the new one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
